### PART 3: Ethics & Optimization

The MNIST handwriting recognition model may exhibit **demographic bias** due to the dataset's composition, which primarily includes digits written by American census workers and high school students in the 1980s–1990s. This underrepresents non-Western handwriting styles, age groups, or cultural writing habits (e.g., left-handed writers, non-native English speakers), potentially reducing accuracy on underrepresented subgroups. Similarly, the Amazon review sentiment model using TextBlob and spaCy NER can introduce **language and selection bias** : reviews in English dominate, excluding multilingual users, while rule-based sentiment lexicons may misclassify sarcasm, negation, or domain-specific terms (e.g., “low sound” interpreted negatively despite context). To mitigate MNIST bias, **TensorFlow Fairness Indicators** can audit performance across protected subgroups (e.g., inferred writer demographics via clustering or external metadata), flagging disparate impact. For spaCy, **custom entity rulers** can be trained on balanced brand/product lists to reduce false negatives, and **sentiment lexicons** can be domain-adapted (e.g., calibrated on verified Amazon reviews) to improve robustness. Both approaches promote fairness through transparency, auditability, and context-aware modeling.
